{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "import os\n",
    "\n",
    "# os.chdir(f'/content/OurDDPM')\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "os.makedirs(\"precomputed\", exist_ok=True)\n",
    "os.makedirs(\"pretrained\", exist_ok=True)\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "os.makedirs(\"runs/interpolation\", exist_ok=True)\n",
    "\n",
    "from ourddpm import OurDDPM\n",
    "from main import dict2namespace\n",
    "import argparse\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pdb\n",
    "import cv2\n",
    "import glob\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "model_path = os.path.join(\"pretrained/celeba_hq.ckpt\")\n",
    "\n",
    "exp_dir = f\"runs/guided\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "n_step =  999#@param {type: \"integer\"}\n",
    "sampling = \"ddpm\" #@param [\"ddpm\", \"ddim\"]\n",
    "fixed_xt = True #@param {type: \"boolean\"}\n",
    "add_var = True #@param {type: \"boolean\"}\n",
    "add_var_on = \"0-999\" #@param {type: \"string\"}\n",
    "vis_gen =  True #@param {type: \"boolean\"}\n",
    "\n",
    "# torch.cuda.set_device(7)\n",
    "\n",
    "args_dic = {\n",
    "    'config': 'celeba.yml', \n",
    "    'n_step': int(n_step), \n",
    "    'sample_type': sampling, \n",
    "    'eta': 0.0,\n",
    "    'bs_test': 1, \n",
    "    'model_path': model_path, \n",
    "    'hybrid_noise': 0, \n",
    "    'align_face': 0,\n",
    "    'image_folder': exp_dir,\n",
    "    'add_var': bool(add_var),\n",
    "    'add_var_on': add_var_on\n",
    "    }\n",
    "args = dict2namespace(args_dic)\n",
    "\n",
    "with open(os.path.join('configs', args.config), 'r') as f:\n",
    "    config_dic = yaml.safe_load(f)\n",
    "config = dict2namespace(config_dic)\n",
    "\n",
    "\n",
    "if bool(add_var):\n",
    "    var_scheduler = []\n",
    "    periods = add_var_on.split(\",\")\n",
    "    for period in periods:\n",
    "        start = int(period.split(\"-\")[0])\n",
    "        end = int(period.split(\"-\")[1])\n",
    "        for n in range(start,end):\n",
    "            var_scheduler.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img + 1) / 2.0\n",
    "\n",
    "def fuse_and_display(imgs):\n",
    "    imgs = [np.array(e) for e in imgs]\n",
    "    imgs = [normalize(img[0].transpose(1, 2, 0)) for img in imgs]\n",
    "    display = cv2.hconcat(imgs)\n",
    "    # display = cv2.cvtColor(display, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    plt.imshow(display)\n",
    "\n",
    "def display_img(img):\n",
    "    img = normalize(img).permute(1, 2, 0)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"data_list\" not in locals():\n",
    "#     with open(\"runs/interpolation/data_1.obj\",\"rb\") as f:\n",
    "#         data_list = pickle.load(f)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:7\")\n",
    "config.device = device\n",
    "runner = OurDDPM(args, config, device=device)\n",
    "runner.load_classifier(\"checkpoint/attr_classifier_4_attrs_40.pt\", feature_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_list = []\n",
    "# for i, s in enumerate([1, 2, 3, 4, 5]):\n",
    "for i, s in enumerate([-200, -100, 0, 100, 200]):\n",
    "    print(f\"Processing {i}th sample...\")\n",
    "\n",
    "    xt = data_list[3][\"xt\"]\n",
    "    noise_traj = torch.tensor(data_list[3][\"noise_traj\"]).cuda()\n",
    "    img = runner.guided_generate_ddpm(xt, var_scheduler, runner.classifier, 1, classifier_scale=s, noise_traj=noise_traj)\n",
    "\n",
    "    res_list.append(img.detach().cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open(\"runs/guided/data_smile_1.obj\", \"wb\") as f:\n",
    "    pickle.dump(res_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with open(\"runs/guided/data_smile_1.obj\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "fuse_and_display(results)\n",
    "classifier_score = []\n",
    "for each in results:\n",
    "    t = (torch.ones(1) * 0).cuda()\n",
    "    classifier_score.append(F.sigmoid(runner.classifier(each.cuda(), t)[:]))\n",
    "classifier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn.functionals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/summertony717/Diffusion-Model-Playground/test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctionals\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m b \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(a)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.functionals'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = F.normalize(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from models.ddpm.diffusion import DDPM\n",
    "from main import dict2namespace\n",
    "import yaml\n",
    "\n",
    "\n",
    "model_path = os.path.join(\"pretrained/celeba_hq.ckpt\")\n",
    "exp_dir = \"PLACEHOLDER\"\n",
    "\n",
    "def get_scheduler(s):\n",
    "    scheduler = set()\n",
    "    periods = s.split(\",\")\n",
    "    for p in periods:\n",
    "        l = int(p.split(\"-\")[0])\n",
    "        r = int(p.split(\"-\")[1])\n",
    "        for i in range(l, r):\n",
    "            scheduler.add(i)\n",
    "    return scheduler\n",
    "\n",
    "n_step =  999#@param {type: \"integer\"}\n",
    "sampling = \"ddpm\" #@param [\"ddpm\", \"ddim\"]\n",
    "fixed_xt = True #@param {type: \"boolean\"}\n",
    "add_var = True #@param {type: \"boolean\"}\n",
    "add_var_on = \"0-999\" #@param {type: \"string\"}\n",
    "vis_gen =  True #@param {type: \"boolean\"}\n",
    "guidance_on  = \"800-999\"\n",
    "\n",
    "var_scheduler = get_scheduler(add_var_on)\n",
    "guidance_scheduler = get_scheduler(guidance_on)\n",
    "\n",
    "args_dic = {\n",
    "    'config': 'celeba.yml', \n",
    "    'n_step': int(n_step), \n",
    "    'sample_type': sampling, \n",
    "    'eta': 0.0,\n",
    "    'bs_test': 1, \n",
    "    'model_path': model_path, \n",
    "    'hybrid_noise': 0, \n",
    "    'align_face': 0,\n",
    "    'image_folder': exp_dir,\n",
    "    'add_var': bool(add_var),\n",
    "    'add_var_on': add_var_on,\n",
    "    'guidance_scheduler': guidance_scheduler\n",
    "    }\n",
    "args = dict2namespace(args_dic)\n",
    "\n",
    "with open(os.path.join('configs', args.config), 'r') as f:\n",
    "    config_dic = yaml.safe_load(f)\n",
    "config = dict2namespace(config_dic)\n",
    "device = torch.device(\"cuda\")\n",
    "config.device = device\n",
    "\n",
    "ckpt_dir = \"checkpoint\"\n",
    "restore_ckpt_dir = os.path.join(ckpt_dir, f\"epoch_{180}.pt\")\n",
    "restoration = DDPM(config)\n",
    "ckpt = torch.load(restore_ckpt_dir)\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in ckpt.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "restoration.load_state_dict(new_state_dict)\n",
    "restoration.conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = 23 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/summertony717/Diffusion-Model-Playground/test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m target \u001b[39m=\u001b[39m [CLIPTarget(target_text)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mwith\u001b[39;00m GradCAM(model\u001b[39m=\u001b[39mclip_model\u001b[39m.\u001b[39mvisual, target_layers\u001b[39m=\u001b[39mtarget_layers, use_cuda\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()) \u001b[39mas\u001b[39;00m cam:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     grayscale_cam \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49marr, targets\u001b[39m=\u001b[39;49mtarget)[\u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[1;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:95\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     84\u001b[0m     loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[39m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# or something else.\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m cam_per_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cam_per_layer(input_tensor,\n\u001b[1;32m     96\u001b[0m                                            targets,\n\u001b[1;32m     97\u001b[0m                                            eigen_smooth)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:134\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    127\u001b[0m     cam \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_cam_image(input_tensor,\n\u001b[1;32m    128\u001b[0m                              target_layer,\n\u001b[1;32m    129\u001b[0m                              targets,\n\u001b[1;32m    130\u001b[0m                              layer_activations,\n\u001b[1;32m    131\u001b[0m                              layer_grads,\n\u001b[1;32m    132\u001b[0m                              eigen_smooth)\n\u001b[1;32m    133\u001b[0m     cam \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(cam, \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m     scaled \u001b[39m=\u001b[39m scale_cam_image(cam, target_size)\n\u001b[1;32m    135\u001b[0m     cam_per_target_layer\u001b[39m.\u001b[39mappend(scaled[:, \u001b[39mNone\u001b[39;00m, :])\n\u001b[1;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m cam_per_target_layer\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/pytorch_grad_cam/utils/image.py:148\u001b[0m, in \u001b[0;36mscale_cam_image\u001b[0;34m(cam, target_size)\u001b[0m\n\u001b[1;32m    146\u001b[0m     img \u001b[39m=\u001b[39m img \u001b[39m/\u001b[39m (\u001b[39m1e-7\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mmax(img))\n\u001b[1;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m target_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(img, target_size)\n\u001b[1;32m    149\u001b[0m     result\u001b[39m.\u001b[39mappend(img)\n\u001b[1;32m    150\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(result)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = 23 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clip_model, preprocess = clip.load(\"RN101\", device=\"cuda\")\n",
    "target = \"woman with blonde hair\"\n",
    "target_text = clip.tokenize([target]).cuda()\n",
    "target_text = clip_model.encode_text(target_text)\n",
    "img = Image.open(\"test.png\")\n",
    "arr = preprocess(img).unsqueeze(0).cuda()\n",
    "print(arr.dtype)\n",
    "\n",
    "class CLIPTarget():\n",
    "    def __init__(self, target_text, clip_reference_text=None):\n",
    "        self.clip_target_text = F.normalize(target_text, dim=-1)\n",
    "        self.clip_reference_text = clip_reference_text\n",
    "        if clip_reference_text is not None:\n",
    "            self.clip_reference_text = F.normalize(clip_reference_text, dim=-1)\n",
    "\n",
    "    def __call__(self, img_embedding):\n",
    "        img = F.normalize(img_embedding, dim=-1)\n",
    "        if self.clip_reference_text is not None:\n",
    "            dot_product = (img @ (self.clip_target_text.T - self.clip_reference_text.T)).squeeze()\n",
    "        else:\n",
    "            dot_product = (img @ self.clip_target_text.T).squeeze()\n",
    "        return dot_product\n",
    "\n",
    "target_layers = [clip_model.visual.layer4[-1]]\n",
    "target = [CLIPTarget(target_text)]\n",
    "with GradCAM(model=clip_model.visual, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n",
    "    grayscale_cam = cam(input_tensor=arr, targets=target)[0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def scale_cam_image(cam, target_size=None):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (1e-7 + np.max(img))\n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py:2813\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2811'>2812</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2812'>2813</a>\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2813'>2814</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 3), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/summertony717/Diffusion-Model-Playground/test.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m a \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39marray(a\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(a)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbolei-gpu01.cs.ucla.edu/home/summertony717/Diffusion-Model-Playground/test.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m img\u001b[39m.\u001b[39mimsave(\u001b[39m\"\u001b[39m\u001b[39mtemp.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py:2815\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2812'>2813</a>\u001b[0m         mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2813'>2814</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2814'>2815</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2815'>2816</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/summertony717/anaconda3/envs/python3.9/lib/python3.9/site-packages/PIL/Image.py?line=2816'>2817</a>\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f4"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"temp.pkl\", \"rb\") as f:\n",
    "    a = pickle.load(f)\n",
    "\n",
    "a = (np.array(a.detach().cpu().squeeze(0).permute(1, 2, 0))+1)/2\n",
    "a = (a * 255).astype(np.uint8)\n",
    "img = Image.fromarray(a)\n",
    "img.imsave(\"temp.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f73c4216640>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.cam_utils import gradCAM\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clip_model, preprocess = clip.load(\"RN101\", device=\"cuda\")\n",
    "target = \"woman with blonde hair\"\n",
    "target_text = clip.tokenize([target]).cuda()\n",
    "target_text = clip_model.encode_text(target_text)\n",
    "img = torch.tensor(a).cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "# arr = preprocess(img).unsqueeze(0).cuda()\n",
    "\n",
    "def clip_normalize(img):\n",
    "    trans = tfs.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    crop = tfs.CenterCrop(224)\n",
    "    return trans(crop(img))\n",
    "\n",
    "CAM = gradCAM(clip_model.visual, clip_normalize(img), target_text, clip_model.visual.layer4)\n",
    "sadboi = np.array(CAM.detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "print(sadboi)\n",
    "plt.figure()\n",
    "plt.imshow(sadboi)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1892d8d4b511927aed5306f7fd835e8c1b51a049a06d2628fb7ee2dd284077c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('python3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1892d8d4b511927aed5306f7fd835e8c1b51a049a06d2628fb7ee2dd284077c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
